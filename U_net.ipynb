{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7mD/PKPWocKF/AtoEncIn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jirapat-tho/my-work/blob/main/U_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKSD5VZzZWG4",
        "outputId": "bfaa4c3b-6b37-42b9-e23e-4254af1cb1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = (\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a0JcbtrbZboG",
        "outputId": "08d14849-337b-436e-ed4d-eb5a072cfabe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "YFO-hI3O31M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# กำหนดเส้นทางไปยังโฟลเดอร์ที่เก็บภาพและ label\n",
        "train_img_dir = '/content/drive/MyDrive/images/train'\n",
        "train_mask_dir = '/content/drive/MyDrive/masks/train'\n",
        "image_files = os.listdir(train_img_dir)\n",
        "\n",
        "# สร้าง X_train (เป็น path ของภาพ) และ y_train (เป็น path ของ mask)\n",
        "X_train = [os.path.join(train_img_dir, file) for file in image_files]\n",
        "y_train = [os.path.join(train_mask_dir, file) for file in image_files]\n",
        "\n",
        "# แบ่งข้อมูลเป็น 90% Train และ 10% Validation\n",
        "\n",
        "train, val, train_mask, val_mask = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# ตรวจสอบจำนวนไฟล์ในแต่ละชุด\n",
        "print(f'Train set size: {len(train)}')\n",
        "print(f'Validation set size: {len(val)}')\n",
        "print(f'Mask train set size: {len(train_mask)}')\n",
        "print(f'Mask validation set size: {len(val_mask)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI6UvwKbZfHl",
        "outputId": "2d4db5c4-20aa-403b-bc83-2385f375128f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 2141\n",
            "Validation set size: 238\n",
            "Mask train set size: 2141\n",
            "Mask validation set size: 238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "test_images_path = '/content/drive/MyDrive/images/test'\n",
        "test_masks_path = '/content/drive/MyDrive/masks/test'\n",
        "\n",
        "# Get list of all files in each folder with full path\n",
        "test_image = {os.path.splitext(filename)[0]: os.path.join(test_images_path, filename)\n",
        "               for filename in os.listdir(test_images_path) if filename.endswith(('.png'))}\n",
        "\n",
        "test_mask = {os.path.splitext(filename)[0]: os.path.join(test_masks_path, filename)\n",
        "              for filename in os.listdir(test_masks_path) if filename.endswith(('.png'))}\n",
        "\n",
        "# Find common keys (filenames without extensions) to match images and masks\n",
        "common_keys = sorted(test_image.keys() & test_mask.keys())\n",
        "\n",
        "# Create lists of matched images and masks paths\n",
        "test = [test_image[key] for key in common_keys]\n",
        "test_mask = [test_mask[key] for key in common_keys]\n",
        "\n",
        "print(\"Number of matched test images:\", len(test_image))\n",
        "print(\"Number of matched test masks:\", len(test_mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk10CZUKZg1P",
        "outputId": "e6e48c8a-cd11-4ca8-fd6c-e3572e4a834d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matched test images: 290\n",
            "Number of matched test masks: 290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ทำ Data Augmentation"
      ],
      "metadata": {
        "id": "qdXl3_mUw5Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2 # Import cv2\n",
        "\n",
        "batch_size=16\n",
        "target_size=(512, 512)\n",
        "\n",
        "# ฟังก์ชันเพิ่ม Data Augmentation\n",
        "def augment_image(image, mask):\n",
        "    # Random Brightness (ค่าความสว่างปรับในช่วง [-0.2, 0.2])\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "\n",
        "    # Random Contrast (ปรับ contrast ในช่วง [0.8, 1.2])\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    # Elastic Transform หรือ Augmentation อื่นสามารถเพิ่มตรงนี้ได้ (หากต้องการ)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# ฟังก์ชันโหลด + Resize\n",
        "def load_and_resize(image_path, mask_path, target_size=(256, 256)): # Define load_and_resize function\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(image, target_size)\n",
        "    image = image / 255.0  # Normalize\n",
        "\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, target_size)\n",
        "    mask = mask / 255.0  # Normalize\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# ฟังก์ชันโหลด + Resize + Augment\n",
        "def load_resize_augment(image_path, mask_path, target_size=(256, 256)):\n",
        "    # Load and resize (ตามโค้ดเดิม)\n",
        "    image, mask = load_and_resize(image_path, mask_path, target_size)\n",
        "\n",
        "    # เพิ่ม Data Augmentation\n",
        "    image, mask = augment_image(image, mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# ฟังก์ชันสร้าง Dataset จาก path ของ images และ masks พร้อม Augmentation\n",
        "def create_dataset_with_augmentation(image_paths, mask_paths, batch_size=16, target_size=(256, 256), is_training=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "\n",
        "    # ใช้ Augmentation เฉพาะตอน training\n",
        "    if is_training:\n",
        "        dataset = dataset.map(lambda x, y: load_resize_augment(x, y, target_size), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda x, y: load_and_resize(x, y, target_size), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# สร้าง dataset สำหรับ train และ validation\n",
        "train_dataset = create_dataset_with_augmentation(train, train_mask, batch_size=batch_size, target_size=target_size, is_training=True)\n",
        "val_dataset = create_dataset_with_augmentation(val, val_mask, batch_size=batch_size, target_size=target_size, is_training=False)"
      ],
      "metadata": {
        "id": "X07yPV2IrWiw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ดีสุดตอนนี้\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout, BatchNormalization, concatenate, Input, Add, Activation, Multiply, AveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "\n",
        "# Attention Gate\n",
        "def attention_gate(skip_connection, gating_signal, filters):\n",
        "    gating = Conv2D(filters, kernel_size=1, padding='same')(gating_signal)\n",
        "    gating = BatchNormalization()(gating)\n",
        "\n",
        "    skip = Conv2D(filters, kernel_size=1, padding='same')(skip_connection)\n",
        "    skip = BatchNormalization()(skip)\n",
        "\n",
        "    attention = Add()([gating, skip])\n",
        "    attention = Activation('relu')(attention)\n",
        "    attention = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(attention)\n",
        "    attention = Multiply()([skip_connection, attention])\n",
        "\n",
        "    return attention\n",
        "\n",
        "# Residual Block\n",
        "def residual_block(input_tensor, filters):\n",
        "    x = Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\")(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)  # เพิ่ม Dropout\n",
        "    x = Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    shortcut = Conv2D(filters, 1, padding=\"same\")(input_tensor)  # Match dimensions\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "# ASPP Block\n",
        "def aspp_block(input_tensor, filters):\n",
        "    pool = AveragePooling2D(pool_size=(2, 2))(input_tensor)\n",
        "    pool = Conv2D(filters, kernel_size=1, padding=\"same\", activation=\"relu\")(pool)\n",
        "    pool = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(pool)\n",
        "\n",
        "    conv1 = Conv2D(filters, kernel_size=1, padding=\"same\", activation=\"relu\")(input_tensor)\n",
        "    conv3 = Conv2D(filters, kernel_size=3, padding=\"same\", dilation_rate=3, activation=\"relu\")(input_tensor)\n",
        "    conv6 = Conv2D(filters, kernel_size=3, padding=\"same\", dilation_rate=6, activation=\"relu\")(input_tensor)\n",
        "\n",
        "    concat = concatenate([pool, conv1, conv3, conv6])\n",
        "    return concat\n",
        "\n",
        "# Improved U-Net with ResNet50 and enhancements\n",
        "def improved_unet_with_enhancements(input_size=(512, 512, 3), l2_value=1e-5, dropout_rate=0.2):\n",
        "    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "\n",
        "    # Encoder (ResNet layers)\n",
        "    encoder_outputs = [\n",
        "        resnet.input,\n",
        "        resnet.get_layer(\"conv1_relu\").output,         # 112x112\n",
        "        resnet.get_layer(\"conv2_block3_out\").output,   # 56x56\n",
        "        resnet.get_layer(\"conv3_block4_out\").output,   # 28x28\n",
        "        resnet.get_layer(\"conv4_block6_out\").output,   # 14x14\n",
        "        resnet.get_layer(\"conv5_block3_out\").output    # 7x7\n",
        "    ]\n",
        "\n",
        "    # Bottleneck with ASPP and Residual Block\n",
        "    center = aspp_block(encoder_outputs[-1], filters=512)\n",
        "    center = residual_block(center, filters=512)\n",
        "\n",
        "    # Decoder with Dropout, L2 Regularization, and BatchNormalization\n",
        "    dec4 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same', kernel_regularizer=l2(l2_value))(center)\n",
        "    dec4 = BatchNormalization()(dec4)\n",
        "    dec4 = Dropout(dropout_rate)(dec4)\n",
        "    dec4 = attention_gate(dec4, encoder_outputs[4], filters=256)\n",
        "    dec4 = concatenate([dec4, encoder_outputs[4]])\n",
        "    dec4 = residual_block(dec4, filters=256)\n",
        "\n",
        "    dec3 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same', kernel_regularizer=l2(l2_value))(dec4)\n",
        "    dec3 = BatchNormalization()(dec3)\n",
        "    dec3 = Dropout(dropout_rate)(dec3)\n",
        "    dec3 = attention_gate(dec3, encoder_outputs[3], filters=128)\n",
        "    dec3 = concatenate([dec3, encoder_outputs[3]])\n",
        "    dec3 = residual_block(dec3, filters=128)\n",
        "\n",
        "    dec2 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same', kernel_regularizer=l2(l2_value))(dec3)\n",
        "    dec2 = BatchNormalization()(dec2)\n",
        "    dec2 = Dropout(dropout_rate)(dec2)\n",
        "    dec2 = attention_gate(dec2, encoder_outputs[2], filters=64)\n",
        "    dec2 = concatenate([dec2, encoder_outputs[2]])\n",
        "    dec2 = residual_block(dec2, filters=64)\n",
        "\n",
        "    dec1 = Conv2DTranspose(32, 2, strides=(2, 2), padding='same', kernel_regularizer=l2(l2_value))(dec2)\n",
        "    dec1 = BatchNormalization()(dec1)\n",
        "    dec1 = Dropout(dropout_rate)(dec1)\n",
        "    dec1 = attention_gate(dec1, encoder_outputs[1], filters=32)\n",
        "    dec1 = concatenate([dec1, encoder_outputs[1]])\n",
        "    dec1 = residual_block(dec1, filters=32)\n",
        "\n",
        "    # Final up-sampling and output layer\n",
        "    outputs = Conv2DTranspose(16, 2, strides=(2, 2), padding='same', kernel_regularizer=l2(l2_value))(dec1)\n",
        "    outputs = BatchNormalization()(outputs)\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', kernel_regularizer=l2(l2_value))(outputs)\n",
        "\n",
        "    model = Model(inputs=resnet.input, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# สร้างโมเดล\n",
        "model = improved_unet_with_enhancements()"
      ],
      "metadata": {
        "id": "p5i0-Htt4ABk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = AdamW(learning_rate= 1e-4, weight_decay=2e-4)"
      ],
      "metadata": {
        "id": "DuETHB1M4RzT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "from tensorflow.keras.backend import epsilon\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "@register_keras_serializable()\n",
        "class IoU(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"iou\", dtype=None, **kwargs):\n",
        "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
        "        self.intersection = self.add_weight(name=\"intersection\", initializer=\"zeros\", dtype=tf.float32)\n",
        "        self.union = self.add_weight(name=\"union\", initializer=\"zeros\", dtype=tf.float32)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred = tf.cast(y_pred > 0.5, dtype=tf.float32)\n",
        "\n",
        "        intersection = tf.reduce_sum(y_true * y_pred)\n",
        "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "\n",
        "        self.intersection.assign_add(intersection)\n",
        "        self.union.assign_add(union)\n",
        "\n",
        "    def result(self):\n",
        "        return self.intersection / (self.union + epsilon())\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.intersection.assign(0.0)\n",
        "        self.union.assign(0.0)\n",
        "\n",
        "@register_keras_serializable()\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + epsilon()) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + epsilon())\n",
        "\n",
        "@register_keras_serializable(package=\"CustomLoss\")\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1e-6\n",
        "    intersection = K.sum(y_true * y_pred)\n",
        "    union = K.sum(y_true) + K.sum(y_pred)\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "    return 1 - dice"
      ],
      "metadata": {
        "id": "2swgYEii4aFF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=dice_loss,metrics=[\"accuracy\", IoU(), dice_coef]) #'binary_crossentropy'"
      ],
      "metadata": {
        "id": "_PILQAFq4cPi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=7, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True,verbose=1)"
      ],
      "metadata": {
        "id": "vNWq28IY4dxB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# สร้าง ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_iou',\n",
        "    save_best_only=True,\n",
        "    mode='max',   # ใช้ 'max' เมื่อค่าสูงกว่าถือว่าดี\n",
        "    verbose=1\n",
        ")\n",
        "# เพิ่มเข้า callback list\n",
        "callbacks = [reduce_lr, early_stopping, checkpoint]"
      ],
      "metadata": {
        "id": "FS_nhlHI4fSl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "izhAPkX34nXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=5, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hRf-Hys4g1M",
        "outputId": "3851e20d-2c16-4620-c763-83ac17f9d77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Model"
      ],
      "metadata": {
        "id": "9K7XTms14qUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "target_size=(512, 512)\n",
        "# ฟังก์ชันโหลดภาพ\n",
        "def load_images(image_paths, target_size=target_size):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path, cv2.IMREAD_COLOR)  # โหลดภาพสี (RGB)\n",
        "        img = cv2.resize(img, target_size)       # Resize เป็น 512x512\n",
        "        img = img / 255.0  # Normalize ค่า pixel ให้เป็น 0-1\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# ฟังก์ชันโหลดมาสก์\n",
        "def load_masks(mask_paths, target_size=target_size):\n",
        "    masks = []\n",
        "    for path in mask_paths:\n",
        "        mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # โหลดมาสก์แบบ Grayscale\n",
        "        mask = cv2.resize(mask, target_size)          # Resize เป็น 512x512\n",
        "        mask = mask / 255.0\n",
        "        mask = np.expand_dims(mask, axis=-1)          # เพิ่มช่องให้เป็น (H, W, 1)\n",
        "        masks.append(mask)\n",
        "    return np.array(masks)\n",
        "\n",
        "# โหลด test images และ masks\n",
        "test_images = load_images(test)\n",
        "test_masks = load_masks(test_mask)\n",
        "\n",
        "print(\"Test Images Shape:\", test_images.shape)\n",
        "print(\"Test Masks Shape:\", test_masks.shape)"
      ],
      "metadata": {
        "id": "dKlV2y5GwEbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"best_model.keras\", custom_objects={\"IoU\": IoU, \"dice_coef\": dice_coef})"
      ],
      "metadata": {
        "id": "yEkJGv1A3u3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/best_model.keras\", custom_objects={\"IoU\": IoU, \"dice_coef\": dice_coef})\n",
        "predicted_masks = model.predict(test_images)\n",
        "predicted_masks = (predicted_masks > 0.5).astype(np.uint8)"
      ],
      "metadata": {
        "id": "MVnQ-Fasvz9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ประเมินผลโมเดล\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "\n",
        "# สร้างตัววัด Mean IoU\n",
        "mean_iou = MeanIoU(num_classes=2)\n",
        "results = model.evaluate(test_images, test_masks, batch_size=1)\n",
        "print(f\"Test Loss: {results[0]:.4f}\")\n",
        "print(f\"Test Accuracy: {results[1]*100:.2f}%\")\n",
        "#print(f\"Test Accuracy: {results[1]:.4f}\")\n",
        "print(f\"Test IoU: {results[2]:.4f}\")\n",
        "print(f\"Test Dice Coefficient: {results[3]:.4f}\")\n",
        "print(\"Mean IoU:\", results[1])"
      ],
      "metadata": {
        "id": "E0u0Y4Yo5RdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}